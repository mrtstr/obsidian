## tokenization
- translating text to numeric data by converting text into discrete units (= **tokens**)
- the vocabulary is a [[set]] of all possible tokens that are mapped to integer IDs with the [[cardinality]] $V$
- note that the tokenizer is trained independently of its language model (e.g. [[nn decoder transformers]]) but the model can only be used with its own tokenizer

### number of tokens vs size of tokens
- small tokens (for example each letter) leads
	- small vocabulary but long sequences per text
	- no unknown words but makes it harder to lean meaning
- large tokens (for example for each word)
	- inflexible when dealing with new words or typos
	- very large vocabulary but shot sequences per text length
	- makes it easier to lean meaning
- solution → sub-word level tokens generated by for example **Byte-Pair Encoding (BPE)**

- helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text
### byte-pair encoding (BPE)
- start with encoding each character as a sequence between 1 and 5 bytes
	- for example, each latter is one byte encoded but special characters are encoded with 5 bytes
- take the complete text and combine the most common occurring byte sequence of length 2 to one token and repeat this until the intended vocabulary size/token size is reached

### special tokens
- `<sos>` / `<bos>`: start of sequence
- `<eos>` : end of sequence that tells the model to stop generating
- `<pad>` : padding token for batch alignment that is ignored by the loss function and not predicted

### chat template tokens
- `<|system|>`: system prompt that sets global behavior or context
- `<|user|>`: user prompt that marks human input
- `<|assistant|>`: assistant prompt that marks model response


# anki


START
Basic
[[nn tokenization]]
- concept
- trade off between small and large tokens
- how the process works
Back: 
## tokenization
- translating text to numeric data by converting text into discrete units (= **tokens**)
- the vocabulary is a [[set]] of all possible tokens that are mapped to integer IDs with the [[cardinality]] $V$
- note that the tokenizer is trained independently of its language model (e.g. [[nn decoder transformers]]) but the model can only be used with its own tokenizer

### number of tokens vs size of tokens
- small tokens (for example each letter) leads
	- small vocabulary but long sequences per text
	- no unknown words but makes it harder to lean meaning
- large tokens (for example for each word)
	- inflexible when dealing with new words or typos
	- very large vocabulary but shot sequences per text length
	- makes it easier to lean meaning
- solution → sub-word level tokens generated by for example **Byte-Pair Encoding (BPE)**

- helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text
### byte-pair encoding (BPE)
- start with encoding each character as a sequence between 1 and 5 bytes
	- for example, each latter is one byte encoded but special characters are encoded with 5 bytes
- take the complete text and combine the most common occurring byte sequence of length 2 to one token and repeat this until the intended vocabulary size/token size is reached

### special tokens
- `<sos>` / `<bos>`: start of sequence
- `<eos>` : end of sequence that tells the model to stop generating
- `<pad>` : padding token for batch alignment that is ignored by the loss function and not predicted

### chat template tokens
- `<|system|>`: system prompt that sets global behavior or context
- `<|user|>`: user prompt that marks human input
- `<|assistant|>`: assistant prompt that marks model response


Tags: ml WS2526
<!--ID: 1761576631646-->
END


START
Basic
[[nn tokenization]]
- spacial tokens with meaning
Back: 

### special tokens
- `<sos>` / `<bos>`: start of sequence
- `<eos>` : end of sequence that tells the model to stop generating
- `<pad>` : padding token for batch alignment that is ignored by the loss function and not predicted

### chat template tokens
- `<|system|>`: system prompt that sets global behavior or context
- `<|user|>`: user prompt that marks human input
- `<|assistant|>`: assistant prompt that marks model response

______________________

## tokenization
- translating text to numeric data by converting text into discrete units (= **tokens**)
- the vocabulary is a [[set]] of all possible tokens that are mapped to integer IDs with the [[cardinality]] $V$
- note that the tokenizer is trained independently of its language model (e.g. [[nn decoder transformers]]) but the model can only be used with its own tokenizer

### number of tokens vs size of tokens
- small tokens (for example each letter) leads
	- small vocabulary but long sequences per text
	- no unknown words but makes it harder to lean meaning
- large tokens (for example for each word)
	- inflexible when dealing with new words or typos
	- very large vocabulary but shot sequences per text length
	- makes it easier to lean meaning
- solution → sub-word level tokens generated by for example **Byte-Pair Encoding (BPE)**

- helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text
### byte-pair encoding (BPE)
- start with encoding each character as a sequence between 1 and 5 bytes
	- for example, each latter is one byte encoded but special characters are encoded with 5 bytes
- take the complete text and combine the most common occurring byte sequence of length 2 to one token and repeat this until the intended vocabulary size/token size is reached



Tags: ml WS2526
<!--ID: 1761576631650-->
END
