### independent component analysis
- rotating the data to recover **independent sources**
- we assume that there exists a [[latent variable]] $Z \sim P_Z \in \mathbb{R}^r$ with [[stochastic independent]] components such that

$$
p_Z(z_1, ..., z_r) = \prod^r_{i=1} p_{Z_i}(z_i)
$$

- The **observed variable** $X \in \mathbb{R}^r$ s generated by a **linear mixing** process via an **unknown invertible mixing matrix** $M \in \mathbb{R}^{r \times r}$

$$
X=MZ
$$

- The goal of [[independent component analysis]] is to **recover the independent components** $Z$ from the observed data $X$ , by learning a **(linear) unmixing matrix** $N \in \mathbb{R}^{r \times r}$ 

$$
\tilde{Z}=NX = NMZ
$$

##### assumptions
- The components of $Z$ are non-Gaussian (except at most one).
- $M$ is invertible
- the data $X$ is centered thus $\mathbb{E}[X]=0$

#### calculating
- first calculate the [[principal component analysis]] such that $P_V(X)=VV^\top X$
- whiten the data such that $\hat{X}=V^\top X$
- find the [[orthogonal matrix]] $R$ that transforms $\hat{X}$ in a way that the [[mutual information]] is minimized

$$
\begin{split}
R = \mathrm{arg} \min_{R^\top R = I} \sum H(\hat{Z_i}) - H(\hat{Z}) \quad \text{with } \hat{Z} = R\hat{X}
\end{split}
$$

#### interpretation
- the [[principal component analysis]] whitens the data such that its components are uncorrelated and unit [[variance]]
- the [[independent component analysis]] rotates the data further to makes the components [[stochastic independent]] (or as independent as possible)


# anki

START
Basic
[[independent component analysis]]
- concept
- assumptions
- how to calculate it
- interpretation compared with the [[principal component analysis]]
Back: 
### independent component analysis
- rotating the data to recover **independent sources**
- we assume that there exists a [[latent variable]] $Z \sim P_Z \in \mathbb{R}^r$ with [[stochastic independent]] components such that

$$
p_Z(z_1, ..., z_r) = \prod^r_{i=1} p_{Z_i}(z_i)
$$

- The **observed variable** $X \in \mathbb{R}^r$ s generated by a **linear mixing** process via an **unknown invertible mixing matrix** $M \in \mathbb{R}^{r \times r}$

$$
X=MZ
$$

- The goal of [[independent component analysis]] is to **recover the independent components** $Z$ from the observed data $X$ , by learning a **(linear) unmixing matrix** $N \in \mathbb{R}^{r \times r}$ 

$$
\tilde{Z}=NX = NMZ
$$

##### assumptions
- The components of $Z$ are non-Gaussian (except at most one).
- $M$ is invertible
- the data $X$ is centered thus $\mathbb{E}[X]=0$

#### calculating
- first calculate the [[principal component analysis]] such that $P_V(X)=VV^\top X$
- whiten the data such that $\hat{X}=V^\top X$
- find the [[orthogonal matrix]] $R$ that transforms $\hat{X}$ in a way that the [[mutual information]] is minimized

$$
\begin{split}
R = \mathrm{arg} \min_{R^\top R = I} \sum H(\hat{Z_i}) - H(\hat{Z}) \quad \text{with } \hat{Z} = R\hat{X}
\end{split}
$$

#### interpretation
- the [[principal component analysis]] whitens the data such that its components are uncorrelated and unit [[variance]]
- the [[independent component analysis]] rotates the data further to makes the components [[stochastic independent]] (or as independent as possible)


_________

### principal component analysis
- represent a [[random vector]] in a lower dimensional [[latent space]] of a given [[dimensions|dimension]] such that it captures the most amount of [[variance]]
- [[principal component analysis]] computes an **orthogonal [[projection]]** of the input data into a lower-dimensional space in order to represent it in a reduced [[latent space]].
- The amount of information retained by the projection can be measured using the **[[explained variance]]**, which quantifies how much of the original variance is preserved.
- let $X \in \mathbb{R}^{d}$ be zero mean [[random variable]] $X \in \mathbb{R}^{n}$ with $\Sigma=\mathbb{VAR} \left[X\right]$ with a [[eigendecomposition|spectral decomposition]]  
- since $\Sigma$ is a [[symmetric matrix]] it is always a [[eigendecomposition]] with a [[orthonormal]] bases, and it admits a [[eigendecomposition]]

$$
\Sigma= \frac{1}{n} X^\top X =U\mathrm{diag}(\lambda_1,..., \lambda_n)U^\top
$$

- with $\lambda_1\geq ...\geq \lambda_d$ being the [[eigenvalue]] and $U=[U_1, ..., U_d] \in \mathbb{R}^{d \times d}$ its [[orthogonal matrix]] of[[eigenvector]]s


- Then the principal components $V_1, \dots, V_r$ that solve the following minimization problems are the [[orthonormal]] [[eigenvector]] of $\Sigma$
- in other words the [[principal component analysis]] is a reduced form of the [[eigendecomposition]] of the empirical [[covariance matrix]]

$$
\begin{split}
V_1 &= \arg\min_{\|v\|=1} \; \mathbb{E} \left[ \left\| X - P_{\mathrm{span}(v)}(X) \right\|_2^2 \right] = \pm U_1 \\
V_r &= \arg\min_{\|v\|=1,\; v \perp V_1, \dots, V_{r-1}} \; \mathbb{E} \left[ \left\| X - P_{\mathrm{span}(u_1, \dots, u_{r-1}, v)}(X) \right\|_2^2 \right] = \pm U_r
\end{split}
$$

- Equivalently, the optimal $r$-dimensional [[subspace]] $V = \mathrm{span}( U_1, \dots, U_r)$ minimizes the [[projection]] error:
- note that $V \in \mathbb{R}^{d \times r}$ is a [[orthonormal matrix]] with $V^\top V = I_r$ but $V V^\top \neq I_d$ in general

$$
\begin{split}
V^{(r)}
&= \arg\min_{\dim(V)=k} \; \mathbb{E} \left[ \left\| X - P_V(X) \right\|_2^2 \right] \\
&= \arg\min_{\dim(V)=k} \; \mathbb{E} \left[ \left\| X \right\|_2^2 \right] - \mathbb{E} \left[ \left\| P_V(X) \right\|_2^2 \right] \\
&= \arg\max_{\dim(V)=k} \; \mathbb{E} \left[ \left\| P_V(X) \right\|_2^2 \right] \\
\end{split}
$$

- this works because of the [[orthogonal|orthogonality]] $\mathbb{E} \left[ \left\| X \right\|_2^2 \right]$ can be decomposed as  follows with $\mathbb{E} \left[ \left\| X \right\|_2^2 \right]$ being constant and thus doesn't matter in an optimization problem
- this means that $V = \mathrm{span}( U_1, \dots, U_r)$ is the $r$ dimensional [[subspace]] that computes the most amount of [[variance]] of the data $X$

$$
\begin{split}
\mathbb{E} \left[ \left\| X \right\|_2^2 \right]
&=\mathbb{E} \left[ \left\| X -  P_V(X) + P_V(X) \right\|_2^2 \right] \\
&=\mathbb{E} \left[ \left\| P_V(X)  \right\|_2^2 \right] + \mathbb{E} \left[ \left\| X -  P_V(X)  \right\|_2^2 \right] +2 \mathbb{E} \left[ \langle X,  X -  P_V(X)  \rangle \right] \\
&=\mathbb{E} \left[ \left\| P_V(X)  \right\|_2^2 \right] + \mathbb{E} \left[ \left\| X -  P_V(X)  \right\|_2^2 \right] \\
\end{split}
$$

- with the following being the projection itself (see [[projection#projection into an orthonormal subspace]]) 

$$
P_V(X) = VV^\top X
$$


### projection into the column space
- [[projection]] of a [[vector]] $b \in \mathbb{R}^n$ to the [[column space]] of [[matrix]] $A \in \mathbb{R}^{n \times d}$ is defined as $P_A(b)=Ax$ 
- $x=\mathrm{arg} \min \|Ax - b \|$ is the coefficient vector that writes the [[projection]]in the coordinates of $A$
- $b - P_A(b)$ has to be [[orthogonal]] to the [[column space]] of $A$ 
- thus the [[projection]] matrix is $P_A = A\left(A^\top A \right)^{-1}A^\top$ and the projection function is $P_A(b) = A\left(A^\top A \right)^{-1}A^\top b$

$$
\begin{split}
\langle A, b-P_A(b) \rangle &= A^\top \left(b-P_A(b)\right) = 0 \\
A^\top P_A(b) &= A^\top b  \\ 
A^\top Ax &= A^\top b  \\ 
x  &= \left(A^\top A \right)^{-1}A^\top b\\
P_A(b)=Ax  &= A \left(A^\top A \right)^{-1}A^\top b\\
\end{split}
$$


![[Pasted image 20221009111212.png]]


### projection into an orthonormal subspace
- given the [[orthonormal]] [[basis]] $V=V_1,..., V_n \in \mathbb{R}^{d \times n}$ with $V^\top V=I$
- let $P_V(X)=Vx$ be the [[projection]] of $X$ into $\mathrm{span}(V_1, ..., V_n)$
- note that $V \in \mathbb{R}^{d \times r}$ is a [[orthonormal matrix]] with $V^\top V = I_r$ but $V V^\top \neq I_d$ in general
- $x=\mathrm{arg} \min \|Vx - X \|$ is the coefficient vector that writes the [[projection]] in the coordinates of $V$

$$
\begin{split}
P_V(X) 
&= V \left(V^\top V \right)^{-1}V^\top X\\
&= V IV^\top X\\
&= V V^\top X\\
\end{split}
$$


Tags: mathematics SS25
<!--ID: 1753277456052-->
END