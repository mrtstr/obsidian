
## general ridge regression
##### model

$$
\begin{split}
Y 
&= X \theta + \epsilon \sim \mathcal{N}(X \theta, I\sigma^2) \\
\end{split}
$$

##### objective function

$$
\begin{split}
\hat\theta_\lambda 
&= arg \min_\theta \frac{1}{n} ||Y - X\theta||^2 + \lambda ||\theta||^2_2 \\
\end{split}
$$
##### solution

- non centered empirical [[empirical covariance]] $\hat\Sigma$ with the [[scalar product]] $\langle a, b\rangle_\hat\Sigma=a^\top\hat\Sigma b$ and the [[norm]]$|| a||_\hat\Sigma^2 =a^\top\hat\Sigma b$ 

$$
\hat\Sigma = \frac{1}{n} X^\top X
$$

$$
\begin{split}
\nabla\left(\frac{1}{n}||Y - X\theta||^2 + \lambda ||\theta||^2_2 \right) 
&= \frac{2}{n} X^\top\left(X\theta -Y\right) + 2\lambda \theta  = 0 \\
0 &= \frac{1}{n} X^\top X\theta - \frac{1}{n} X^\top Y + \lambda \theta \\
\frac{1}{n}X^\top Y&=\left(\frac{1}{n}X^\top X + \lambda I \right)\theta  \\
\Rightarrow\hat\theta_\lambda 
&=\frac{1}{n}\left(\frac{1}{n}X^\top X + \lambda I \right)^{-1}X^\top Y \\
&=\left(X^\top X + n \lambda I \right)^{-1}X^\top Y \\
&=\frac{1}{n}\left(\hat\Sigma + \lambda I \right)^{-1}X^\top Y \\
\end{split}
$$

##### expectation

$$
\begin{split}
\mathbb{E}\left[\hat\theta_\lambda\right] 
&= \frac{1}{n}\left(\hat\Sigma + \lambda I \right)^{-1} X^\top \mathbb{E}[Y] \\
&= \frac{1}{n}\left(\hat\Sigma + \lambda I \right)^{-1}X^\top  X \theta\\
&= \left(\hat\Sigma + \lambda I \right)^{-1}\hat\Sigma \theta\\
&= \theta - \lambda \left(\hat\Sigma + \lambda I \right)^{-1}\theta\\
\end{split}
$$

##### variance
$$
\begin{split}
\mathbb{VAR}\left[ \hat\theta_\lambda\right] 
&=\mathbb{E}\left[\left(\hat\theta_\lambda-\mathbb{E}\left[\hat\theta_\lambda\right]\right)\left(\hat\theta_\lambda-\mathbb{E}\left[\hat\theta_\lambda\right]\right)^\top\right] \\
&=\mathbb{E}\left[\frac{1}{n^2}\left(\hat\Sigma + \lambda I \right)^{-1} X^\top \left(Y-\mathbb{E}\left[Y\right]\right)\left(Y-\mathbb{E}\left[Y\right]\right)^\top X\left(\hat\Sigma + \lambda I \right)^{-1} \right] \\
&=\mathbb{E}\left[\frac{1}{n^2}\left(\hat\Sigma + \lambda I \right)^{-1} X^\top \mathbb{VAR}[Y] X\left(\hat\Sigma + \lambda I \right)^{-1} \right] \\
&=\frac{1}{n^2}\left(\hat\Sigma + \lambda I \right)^{-1} X^\top I \sigma^2 X\left(\hat\Sigma + \lambda I \right)^{-1}  \\
&=\frac{\sigma^2}{n^2}\left(\hat\Sigma + \lambda I \right)^{-1} X^\top  X\left(\hat\Sigma + \lambda I \right)^{-1}  \\
&=\frac{\sigma^2}{n}\left(\hat\Sigma + \lambda I \right)^{-1} \hat\Sigma\left(\hat\Sigma + \lambda I \right)^{-1}  \\
\end{split}
$$
### Parameter recovery theory
- We assume the data is generated by

$$
\begin{split}
Y 
&= X \theta^* + \epsilon \sim \mathcal{N}(X \theta, I\sigma^2) \\
\end{split}
$$


- focus on the parameter error $\mathrm{MSE}(\theta_{\lambda})$ instead of the excess risk of the output
- with the following bias variance decomposition

$$
\begin{split}
\mathrm{MSE}(\theta_{\lambda}) 
&=  \mathbb{E}\left[ \left|\left| \theta_{\lambda} - \theta^*    \right|\right|^2_2 \right]   \\
&=  \mathbb{E}\left[ \left|\left| \mathbb{E}\left[\hat\theta_\lambda\right] - \theta^* + \mathbb{E}\left[\hat\theta_\lambda\right] -  \theta_{\lambda} \right|\right|^2_2 \right]   \\
&=   \left|\left|  \mathbb{E}\left[\hat\theta_\lambda\right] - \theta^* \right|\right|^2_2  + \mathbb{E}\left[ \left|\left|\mathbb{E}\left[\hat\theta_\lambda\right] -   \hat\theta_\lambda\right|\right|^2_2 \right]  \\
&=   \left|\left|  \mathbb{E}\left[\hat\theta_\lambda\right] - \theta^* \right|\right|^2_2  + \mathbb{E}\left[ \left|\left|\mathbb{E}\left[\hat\theta_\lambda\right] -   \hat\theta_\lambda\right|\right|^2_2 \right]  \\
&= \left|\left| \lambda \left(\hat\Sigma + \lambda I \right)^{-1}\theta^*  \right|\right|^2_2  + \frac{\sigma^2}{n} \mathrm{TR}\left(\hat\Sigma\left(\hat\Sigma + \lambda I \right)^{-2} \right) \\
\end{split}
$$

- let $\hat\Sigma=U\Lambda U^\top$ be the [[singular value decomposition]] with $\Lambda=\mathrm{diga}(\lambda_1, ..., \lambda_d)$
- since $U$ is a orthonormal bases $\theta^*$ can be expressed as $\theta^*=U\beta$ with $\beta=U^\top \theta^*$ 
- the bias contribution to the MSE becomes 

$$
\begin{split}
\left|\left| \lambda \left(\hat\Sigma + \lambda I \right)^{-1}\theta^*  \right|\right|^2_2
&=  \left|\left| \lambda \left(U\Lambda U^\top + \lambda I \right)^{-1}U\beta  \right|\right|^2_2   \\
&=  \left|\left| \lambda \left(U\Lambda U^\top + \lambda UU^\top \right)^{-1}U\beta  \right|\right|^2_2   \\
&=  \left|\left| \lambda U\left(\Lambda + \lambda I \right)^{-1}U^\top U\beta  \right|\right|^2_2   \\
&= \lambda^2 \left|\left| \left(\Lambda + \lambda I \right)^{-1}\beta  \right|\right|^2_2   \\
&= \lambda^2 \sum_{i=1}^d \left( \frac{\beta_i}{\lambda_i+\lambda}\right)^2  \\
\end{split}
$$

$$
\begin{split}
\frac{\sigma^2}{n} \mathrm{TR}\left(\hat\Sigma\left(\hat\Sigma + \lambda I \right)^{-2} \right)
&=  \frac{\sigma^2}{n} \mathrm{TR}\left(U\Lambda U^\top\left(U\Lambda U^\top + \lambda I \right)^{-2} \right)   \\
&=  \frac{\sigma^2}{n} \mathrm{TR}\left(U\Lambda U^\top\left(U\Lambda U^\top + \lambda UU^\top \right)^{-2} \right)   \\
&=  \frac{\sigma^2}{n} \mathrm{TR}\left(U\Lambda U^\top U^2\left(\Lambda  + \lambda I \right)^{-2}U^{-2} \right)   \\
&=  \frac{\sigma^2}{n} \mathrm{TR}\left(\Lambda \left(\Lambda + \lambda I \right)^{-2} \right)   \\
&=  \frac{\sigma^2}{n} \sum_{i=1}^d  \frac{\lambda_i}{\left(\lambda_i+\lambda\right)^2}    \\
\end{split}
$$

there is not closed solution in general but it can be solved numericly

$$
\begin{split}
\lambda^{*}_{\mathrm{num}}
&= arg \min_\lambda  \mathrm{MSE}(\theta_{\lambda})  \\
\end{split}
$$

- with the following assumtions there exists a closed solution



1) $\hat\Sigma=I$ such that all eigenvalues $\lambda_i=1$
2) all $\beta_i$ are equal $\theta^*=\beta \sum u_i$

generally true $\sum \beta_i = ||\theta^*||^2$ because $||\theta^*||^2= ||U\beta||^2= ||\beta||^2 = \sum \beta_i^2$


$$
\begin{split}
\mathrm{MSE}(\theta_{\lambda}) 
&=  \lambda^2 \sum_{i=1}^d \left( \frac{\beta_i}{\lambda_i+\lambda}\right)^2 +  \frac{\sigma^2}{n} \sum_{i=1}^d  \frac{\lambda_i}{\left(\lambda_i+\lambda\right)^2}  \\
&=  \lambda^2  \frac{d\beta_1^2}{(1+\lambda)^2} +  \frac{\sigma^2d}{n}   \frac{1}{\left(1+\lambda\right)^2}  \\
&=  \lambda^2  \frac{||\theta^*||^2}{(1+\lambda)^2} +  \frac{\sigma^2d}{n}   \frac{1}{\left(1+\lambda\right)^2}  \\
\end{split}
$$

- try to find the regularization parameter $\lambda^{*}_{\mathrm{param}}$ that minimizes

$$
\begin{split}
\frac{d}{d\lambda} \mathrm{MSE}(\theta_{\lambda}) 
&= 0 \\
&= \frac{2\lambda||\theta^*||^2(1+\lambda)^2-2\left(1+\lambda\right)\left(\lambda^2 ||\theta^*||^2 - \frac{\sigma^2d}{n}  \right)}{(1+\lambda)^4} \\
&= \lambda||\theta^*||^2(1+\lambda)-\lambda^2 ||\theta^*||^2 - \frac{\sigma^2d}{n}   \\
&= \lambda||\theta^*||^2+\lambda^2||\theta^*||^2-\lambda^2 ||\theta^*||^2 - \frac{\sigma^2d}{n}   \\
&= \lambda||\theta^*||^2 - \frac{\sigma^2d}{n}   \\
\Rightarrow \lambda^{*}_{\mathrm{closed}} &= \frac{\sigma^2d}{n||\theta^*||^2}
\end{split}
$$

## Simulation
### random parameters

##### single trail
![/home/martin/repos/stock_prediction/plots/ridge_mse_plot_single_2025-06-14_19-01-53.svg](file:///home/martin/repos/stock_prediction/plots/ridge_mse_plot_single_2025-06-14_19-01-53.svg)

##### multiple trails

![/home/martin/repos/stock_prediction/plots/ridge_mse_plot_multi_2025-06-14_19-02-44.svg](file:///home/martin/repos/stock_prediction/plots/ridge_mse_plot_multi_2025-06-14_19-02-44.svg)

### Isotropic coefficient vector
##### single trail
![/home/martin/repos/stock_prediction/plots/ridge_mse_plot_multi_2025-06-14_19-08-40.svg](file:///home/martin/repos/stock_prediction/plots/ridge_mse_plot_multi_2025-06-14_19-08-40.svg)


##### multiple trails
![/home/martin/repos/stock_prediction/plots/ridge_mse_plot_multi_2025-06-14_19-08-00.svg](file:///home/martin/repos/stock_prediction/plots/ridge_mse_plot_multi_2025-06-14_19-08-00.svg)


$$
\begin{split}
\mathbb{E}_{\epsilon, X}\left[\mathrm{MSE}(\theta_{\lambda}) \right] 
&= 
\end{split}
$$





- calculate analytically the minimum of the exception of the MSE taken over X and epsilon for a given theta
