### pretraining
- phase one in the [[LLM training]]
- Train on very large, general text-based datasets (web, books, code, etc.).
- Objective:
  - **Base model with broad language capabilities**
  - Learns patterns of grammar, discourse, reasoning, and factual associations.
- Extremely compute-intensive and expensive.