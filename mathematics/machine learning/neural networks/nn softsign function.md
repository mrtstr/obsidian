### softsign function
- the [[nn softsign function]] can be used as [[nn activation function]] for [[neural network]] and is defined a $\mathrm{softsign}: \mathbb{R} \to (-1, 1)$

$$
\mathrm{softsign}(z) = \frac{z}{1+|z|}
$$

- similar shape to the [[nn hyperbolic tangent]] in a sense that its mapping to $(-1, 1)$ and is centered around zero but is smoother
	â†’ computationally simpler and fewer problems with vanishing gradients