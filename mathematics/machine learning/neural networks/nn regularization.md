## regularization
- during training of a [[neural network]] we are minimizing the training [[risk]] but at the same time we also want the training error ([[risk]] on the training set) to be not too much different from the true [[risk]] (based on the true [[data distribution]])
- [[regularization]] methods reduce the discrepancy between the error on the training dataset and the true error 