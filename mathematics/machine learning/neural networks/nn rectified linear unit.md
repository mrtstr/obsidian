### rectified linear unit
- most widely used [[nn activation function]] for hidden layers of [[neural network]] 
- $\mathrm{ReLu}: \mathbb{R} \to \mathbb{R}^+$ is defined as follows

$$
\mathrm{ReLu}(z)=\max(0, z)
$$